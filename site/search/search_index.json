{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MAS Performance Wiki \uf0c1 Info This site will be updated periodically. More topics will be added soon. Lab benchmarks are not published, but can be shared upon request, with completion of an NDA. This site provides best practices, sizing and troubleshooting guidelines to improve the performance of IBM Maximo Application Suite (MAS) . Maximo 7.x Best Practices are also available on the site. Most DB configurations in the best practice are still applicable to MAS Manage app.","title":"Home"},{"location":"#welcome-to-mas-performance-wiki","text":"Info This site will be updated periodically. More topics will be added soon. Lab benchmarks are not published, but can be shared upon request, with completion of an NDA. This site provides best practices, sizing and troubleshooting guidelines to improve the performance of IBM Maximo Application Suite (MAS) . Maximo 7.x Best Practices are also available on the site. Most DB configurations in the best practice are still applicable to MAS Manage app.","title":"Welcome to MAS Performance Wiki"},{"location":"mas/aws/bestpractice/","text":"AWS \uf0c1 Instance Type \uf0c1 There are many instance type available in AWS. Based on the benchmark, recommend M5, M6 instances (e.g.M5.4xlarge) as master or worker nodes and P3, P4 as GPU nodes. Note Depending on the regions, some instances may not be available. Use AWS Pricing Calculator to check the instance availability and cost. g4dn can be used as GPU node for test/dev env, but not recommended for production env. If the application requires a good network performance, check Amazon EC2 instance network bandwidth site for more details. For production env, an instance with 10GB ethernet is recommended. Classic Load Balancer Idle Timeout \uf0c1 Each OCP cluster creates 1 class load balancer and 2 network load balancers in AWS. AWS classic load balancer has a default idle time 60 seconds. In some cases, this value is not enough for a long time transaction e.g. asset health check notebook. Consider to adjust this value to what the application needs e.g. 300 seconds. Also, monitoring classic load-balance performance is strongly recommend, particularly with IoT related app. (Note: Surge Queue Length's default/hard code limit is 1024 . when queue is fully, the tcp handshake will fail) Amazon DocumentDB \uf0c1 DocumentDB that is a fully managed MongoDB compatibility database. It can be used by both IBM Suite License Service (SLS) and MAS Core. However, there are functional differences between DocumentDB and MongoDB. Check this link for more details. Note: When using DocumentDB, it requires to set RetryWrite=false in SLS and Suite CRs. Amazon MSK \uf0c1 MAS supports MSK which is a fully managed apache Kafka service. Note monitor MSK performance via CloudWatch is strongly recommended. Key metrics include Disk usage by broker, CPU (User) usage by broker, Active Controller Count, Network RX packets by broker, Network TX packets by broker . define an appropriate config for Kafka, MSK and topics. e.g. retention.ms, retention.bytes, partitions and replics to support the workload. AWS Storage \uf0c1 EBS storages like gp2, gp3 is supported by OCP in AWS. Note: EBS storage is ReadWriteOnce. The volume can be mounted as read-write by a single node. io1 and io2 are SSD-based EBS that provides the higher performance. Check Amazon EBS volume types for extra info like throughput, tuning and cost. Below is a sample yaml to create io1 storageclass with 100 iopsPerGB . kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : io1 provisioner : kubernetes.io/aws-ebs parameters : encrypted : 'true' iopsPerGB : '100' type : io1 reclaimPolicy : Delete allowVolumeExpansion : true volumeBindingMode : Immediate EFS Storage can be used as ReadWriteMany storageclass. EFS has different metered throughput modes. Bursting Throughput mode is the default. It is inexpensive, but does NOT perform well if all burst credits are used. Monitor BurstCreditBalance metric in CloudWatch. Provisioned Throughput mode is relatively expensive. It can drive up to 3 GiBps for read operations and 1 GiBps for write operations per file system More info can be found at Amazon EFS performance Self-managed OCP vs AWS ROSA \uf0c1 A self-managed OCP Cluster can be created by the installer cli tool that supports both IPI and UPI mode. It requires self maintenance and upgrades. Alternatively, ROSA is a managed Red Hat OpenShift Service. Each ROSA cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and Amazon support.","title":"AWS"},{"location":"mas/aws/bestpractice/#aws","text":"","title":"AWS"},{"location":"mas/aws/bestpractice/#instance-type","text":"There are many instance type available in AWS. Based on the benchmark, recommend M5, M6 instances (e.g.M5.4xlarge) as master or worker nodes and P3, P4 as GPU nodes. Note Depending on the regions, some instances may not be available. Use AWS Pricing Calculator to check the instance availability and cost. g4dn can be used as GPU node for test/dev env, but not recommended for production env. If the application requires a good network performance, check Amazon EC2 instance network bandwidth site for more details. For production env, an instance with 10GB ethernet is recommended.","title":"Instance Type"},{"location":"mas/aws/bestpractice/#classic-load-balancer-idle-timeout","text":"Each OCP cluster creates 1 class load balancer and 2 network load balancers in AWS. AWS classic load balancer has a default idle time 60 seconds. In some cases, this value is not enough for a long time transaction e.g. asset health check notebook. Consider to adjust this value to what the application needs e.g. 300 seconds. Also, monitoring classic load-balance performance is strongly recommend, particularly with IoT related app. (Note: Surge Queue Length's default/hard code limit is 1024 . when queue is fully, the tcp handshake will fail)","title":"Classic Load Balancer Idle Timeout"},{"location":"mas/aws/bestpractice/#amazon-documentdb","text":"DocumentDB that is a fully managed MongoDB compatibility database. It can be used by both IBM Suite License Service (SLS) and MAS Core. However, there are functional differences between DocumentDB and MongoDB. Check this link for more details. Note: When using DocumentDB, it requires to set RetryWrite=false in SLS and Suite CRs.","title":"Amazon DocumentDB"},{"location":"mas/aws/bestpractice/#amazon-msk","text":"MAS supports MSK which is a fully managed apache Kafka service. Note monitor MSK performance via CloudWatch is strongly recommended. Key metrics include Disk usage by broker, CPU (User) usage by broker, Active Controller Count, Network RX packets by broker, Network TX packets by broker . define an appropriate config for Kafka, MSK and topics. e.g. retention.ms, retention.bytes, partitions and replics to support the workload.","title":"Amazon MSK"},{"location":"mas/aws/bestpractice/#aws-storage","text":"EBS storages like gp2, gp3 is supported by OCP in AWS. Note: EBS storage is ReadWriteOnce. The volume can be mounted as read-write by a single node. io1 and io2 are SSD-based EBS that provides the higher performance. Check Amazon EBS volume types for extra info like throughput, tuning and cost. Below is a sample yaml to create io1 storageclass with 100 iopsPerGB . kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : io1 provisioner : kubernetes.io/aws-ebs parameters : encrypted : 'true' iopsPerGB : '100' type : io1 reclaimPolicy : Delete allowVolumeExpansion : true volumeBindingMode : Immediate EFS Storage can be used as ReadWriteMany storageclass. EFS has different metered throughput modes. Bursting Throughput mode is the default. It is inexpensive, but does NOT perform well if all burst credits are used. Monitor BurstCreditBalance metric in CloudWatch. Provisioned Throughput mode is relatively expensive. It can drive up to 3 GiBps for read operations and 1 GiBps for write operations per file system More info can be found at Amazon EFS performance","title":"AWS Storage"},{"location":"mas/aws/bestpractice/#self-managed-ocp-vs-aws-rosa","text":"A self-managed OCP Cluster can be created by the installer cli tool that supports both IPI and UPI mode. It requires self maintenance and upgrades. Alternatively, ROSA is a managed Red Hat OpenShift Service. Each ROSA cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and Amazon support.","title":"Self-managed OCP vs AWS ROSA"},{"location":"mas/core/bestpractice/","text":"MAS Core \uf0c1 Workload Scaling ConfigMap \uf0c1 Since 8.10, MAS core allows users to customize resource settings for cpu, memory and replica counts per component. By default MAS ships with three default configmaps for workload scaling: {mas_instance_id}-wl-cust-small {mas_instance_id}-wl-cust-medium {mas_instance_id}-wl-cust-large You can modify the default configmaps or create a custom one if needed. Should you decide to create a custom workload scaling configmap, it must be specified using the MAS Suite CR . Below is a snippet of a custom configmap for workload scaling. You should understand what you are doing before you change the default values, as decreasing resources can have undesirable effects. coreidpcfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi slscfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi Tip high workload impact pods: core-ipd, core-api, license-mediator check MAS Pods Explained to understand the insight of each pod functionality check the health of mongodb and sls api-licensing pods (in the SLS namespace) if any performance issue is detected","title":"MAS Core"},{"location":"mas/core/bestpractice/#mas-core","text":"","title":"MAS Core"},{"location":"mas/core/bestpractice/#workload-scaling-configmap","text":"Since 8.10, MAS core allows users to customize resource settings for cpu, memory and replica counts per component. By default MAS ships with three default configmaps for workload scaling: {mas_instance_id}-wl-cust-small {mas_instance_id}-wl-cust-medium {mas_instance_id}-wl-cust-large You can modify the default configmaps or create a custom one if needed. Should you decide to create a custom workload scaling configmap, it must be specified using the MAS Suite CR . Below is a snippet of a custom configmap for workload scaling. You should understand what you are doing before you change the default values, as decreasing resources can have undesirable effects. coreidpcfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi slscfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi Tip high workload impact pods: core-ipd, core-api, license-mediator check MAS Pods Explained to understand the insight of each pod functionality check the health of mongodb and sls api-licensing pods (in the SLS namespace) if any performance issue is detected","title":"Workload Scaling ConfigMap"},{"location":"mas/ibmcloud/bestpractice/","text":"IBM Cloud \uf0c1 IBM Storage \uf0c1 IBM Cloud provides both block and file storages for OCP. Both storages support ReadWriteMany access. If the app requires a high-performance disks, consider to setup custom performance storageclass as blow: block storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : block100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-block reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer file storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : file100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-file reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer IBM External Load Balancer \uf0c1 If the built-in ingress load balancer in OCP is unable to scale to handle with \"large\" workloads (100K+ concurrent device connections), consider to provision an instance of IBM cloud NLB2.0 (IPVS/KeepAlived) load balancer. IBM ROKS \uf0c1 IBM ROKS is a managed Red Hat OpenShift Service in IBM Cloud. Each ROKS cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and IBM Cloud support.","title":"IBM Cloud"},{"location":"mas/ibmcloud/bestpractice/#ibm-cloud","text":"","title":"IBM Cloud"},{"location":"mas/ibmcloud/bestpractice/#ibm-storage","text":"IBM Cloud provides both block and file storages for OCP. Both storages support ReadWriteMany access. If the app requires a high-performance disks, consider to setup custom performance storageclass as blow: block storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : block100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-block reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer file storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : file100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-file reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer","title":"IBM Storage"},{"location":"mas/ibmcloud/bestpractice/#ibm-external-load-balancer","text":"If the built-in ingress load balancer in OCP is unable to scale to handle with \"large\" workloads (100K+ concurrent device connections), consider to provision an instance of IBM cloud NLB2.0 (IPVS/KeepAlived) load balancer.","title":"IBM External Load Balancer"},{"location":"mas/ibmcloud/bestpractice/#ibm-roks","text":"IBM ROKS is a managed Red Hat OpenShift Service in IBM Cloud. Each ROKS cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and IBM Cloud support.","title":"IBM ROKS"},{"location":"mas/iot/bestpractice/","text":"MAS IoT \uf0c1 IoT Deployment \uf0c1 IoT CRD defines 3 default size deployments: dev, small, medium that controls the default settings for pod replics, cpu and memory. For production, medium is required. Sample yaml for medium deployment in IoT CR apiVersion : iot.ibm.com/v1 kind : IoT metadata : name : masinst1 namespace : mas-masinst1-iot spec : bindings : jdbc : system kafka : system mongo : system settings : deployment : size : medium If need to adjust the default setting for a deployment, go the iot-operator pod, then change the corresponding yaml files under /opt/ansible/roles/<ibm-iot-operator>/vars folder, e.g. /opt/ansible/roles/ibm-iot-actions/vars/size_medium.yml Connection and OpenShift Ingress Controllers \uf0c1 Openshift HAProxy supports 20k connection per pod. The total connection determinants how many end devices can connect to IoT MSProxy. By default, IBM ROKS deploys 3 router members that supports 3x20k = 60K connection By default, AWS ROSA deploys 2 router members that supports 2x20k = 40K connection use the below command to scale up to 3 router members: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge Kafka \uf0c1 IoT uses Kafka to process the messages. Follow the Kafka Configuration Reference to configure best value for Kafka/Topics retention.ms, retention.bytes, partitions, replics to support the workload. AWS MSK \uf0c1 configuration details can be found at https://docs.aws.amazon.com/msk/latest/developerguide/msk-default-configuration.html monitoring MSK is strongly recommended monitoring classic load-balance is strongly recommend Message Rate and Ethernet Network Bandwidth \uf0c1 Depends on the cloud providers, worker node instance has difference network bandwidth. It determinants how fast the end devices can send the request. Message rate is limited by the message size and the bandwidth of ethernet network. To achieve higher rates and/or larger messages it will require a 10GB ethernet. The network bandwidth also impacts the response latency. The higher bandwidth, the lower latency. Below deployment configurations are recommended as starting value with medium and large workload. MSProxy - 4 MSProxies with 1 CPU and 4GB MessageGateWay - 1 MGW 6 CPUs and 16GB along with 4 TcpIop threads Modify TcpThread in MessageGateWay(MGW) pod \uf0c1 edit /var/messagesight/data/config/server.cfg add a line TcpThreads = 4 do the above changes on all MGW Pod, then restart the pods","title":"MAS IoT"},{"location":"mas/iot/bestpractice/#mas-iot","text":"","title":"MAS IoT"},{"location":"mas/iot/bestpractice/#iot-deployment","text":"IoT CRD defines 3 default size deployments: dev, small, medium that controls the default settings for pod replics, cpu and memory. For production, medium is required. Sample yaml for medium deployment in IoT CR apiVersion : iot.ibm.com/v1 kind : IoT metadata : name : masinst1 namespace : mas-masinst1-iot spec : bindings : jdbc : system kafka : system mongo : system settings : deployment : size : medium If need to adjust the default setting for a deployment, go the iot-operator pod, then change the corresponding yaml files under /opt/ansible/roles/<ibm-iot-operator>/vars folder, e.g. /opt/ansible/roles/ibm-iot-actions/vars/size_medium.yml","title":"IoT Deployment"},{"location":"mas/iot/bestpractice/#connection-and-openshift-ingress-controllers","text":"Openshift HAProxy supports 20k connection per pod. The total connection determinants how many end devices can connect to IoT MSProxy. By default, IBM ROKS deploys 3 router members that supports 3x20k = 60K connection By default, AWS ROSA deploys 2 router members that supports 2x20k = 40K connection use the below command to scale up to 3 router members: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge","title":"Connection and OpenShift Ingress Controllers"},{"location":"mas/iot/bestpractice/#kafka","text":"IoT uses Kafka to process the messages. Follow the Kafka Configuration Reference to configure best value for Kafka/Topics retention.ms, retention.bytes, partitions, replics to support the workload.","title":"Kafka"},{"location":"mas/iot/bestpractice/#aws-msk","text":"configuration details can be found at https://docs.aws.amazon.com/msk/latest/developerguide/msk-default-configuration.html monitoring MSK is strongly recommended monitoring classic load-balance is strongly recommend","title":"AWS MSK"},{"location":"mas/iot/bestpractice/#message-rate-and-ethernet-network-bandwidth","text":"Depends on the cloud providers, worker node instance has difference network bandwidth. It determinants how fast the end devices can send the request. Message rate is limited by the message size and the bandwidth of ethernet network. To achieve higher rates and/or larger messages it will require a 10GB ethernet. The network bandwidth also impacts the response latency. The higher bandwidth, the lower latency. Below deployment configurations are recommended as starting value with medium and large workload. MSProxy - 4 MSProxies with 1 CPU and 4GB MessageGateWay - 1 MGW 6 CPUs and 16GB along with 4 TcpIop threads","title":"Message Rate and Ethernet Network Bandwidth"},{"location":"mas/iot/bestpractice/#modify-tcpthread-in-messagegatewaymgw-pod","text":"edit /var/messagesight/data/config/server.cfg add a line TcpThreads = 4 do the above changes on all MGW Pod, then restart the pods","title":"Modify TcpThread in MessageGateWay(MGW) pod"},{"location":"mas/manage/bestpractice/","text":"MAS Manage \uf0c1 App Server \uf0c1 MAS Manage has different bundle types e.g. All, UI, MEA, Report and CRON to configure app server. Adjust the resource settings like cpu, memory, replic to match the workload. The setting is in ManageWorkspaces CR. Below is the sample. apiVersion : apps.mas.ibm.com/v1 kind : ManageWorkspace ... spec : settings : deployment : serverBundles : - bundleType : mea isDefault : false isMobileTarget : false isUserSyncTarget : true name : mea replica : 1 routeSubDomain : all - bundleType : cron isDefault : false isMobileTarget : false isUserSyncTarget : false name : cron replica : 1 ... Load-Balancer \uf0c1 Lab test shows roundrobin has more stable and better performance than leastconn policy which is the default. Follow this link to update load balance policy. Manage Pod Functionality \uf0c1 Follow this page to understand the manage pod functionality. LTPA timeout \uf0c1 Using IBM Maximo Application Suite (MAS), Manage users will receive an error message saying to reload the application after 2 hours, even while actively working. This 2-hour timeout default is when the LTPA token in Manage expires, and is redirecting the user back to the login page for MAS. Follow Updating LTPA timeout in Manage to increase the default value. WebSphere Liberty \uf0c1 Due to the architecture change, Maximo 8.x (MAS Manage app) is deployed on WebSphere Liberty Base 21.0.0.5 with OpenJ9. As of WebSphere Liberty 18.0.0.1, the thread growth algorithm is enhanced to grow thread capacity more aggressively so as to react more rapidly to peak loads. For many environments, this autonomic tuning provided by the Open Liberty thread pool works well with no configuration or tuning by the operator. If necessary, you can adjust coreThreads and maxThreads value. Follow this page for tuning liberty Configure JVM options in Manage app \uf0c1 Follow this page to configure JVM options DB \uf0c1 Disk \uf0c1 disk performance is critial for db performance. Recommend a storage or disk with disk throughput > 200 MB/s storage class: recommend a storage with 100+ IOPS DB - DB2/DB2wh \uf0c1 DB2 Tuning in Maximo 7.6.x Best practice is applicable. Highlights: increase maxsequence cache to 50 run runstats and/or reorg to update index periodically separate system storage, user storage, backup storage, transaction logs storage, temporary tablespace storage on as different disks if possible. Use DB2 Performance Diagnosis to troubleshoot and tuning the db and SQL. Manage requires row-organized tables. Check db2w db setting (by default it uses column based) and update the setting by db2 update db cfg using DFT_TABLE_ORG ROW Manage does NOT support MMP or tale partition in the current version, but consider to archive records over 1-year old. Optim is the one of the tools can be used for archiving. see this guide and this video for details. Increase the concurrently running statements allowed for a DB2 application. This issue occcurs when loading a large amount of data via MIF or api call. See this link for the tuning. storageclass for ibm cloud: performance(Custom) block storage with 100+ IOPS for data storage, block gold for system and block silver for backup for aws cloud: if using EFS for db, consider Provisioned mode to have a constant throughput. For more disk options, see details in this page DB - Oracle \uf0c1 Maximo 7.6.x Best practice is applicable DB - MSSQL \uf0c1 Maximo 7.6.x Best practice is applicable additional settings for MSSQL Server 2019 compatibility level: if maximo db is upgraded from the old version and the performance degradtation is observed after the upgrade, consider to set compatibility level to the old version to keep the execution plan same. isolation level: ALTER DATABASE < DB NAME > SET ALLOW_SNAPSHOT_ISOLATION ON ALTER DATABASE < DB NAME > SET READ_COMMITTED_SNAPSHOT ON","title":"MAS Manage"},{"location":"mas/manage/bestpractice/#mas-manage","text":"","title":"MAS Manage"},{"location":"mas/manage/bestpractice/#app-server","text":"MAS Manage has different bundle types e.g. All, UI, MEA, Report and CRON to configure app server. Adjust the resource settings like cpu, memory, replic to match the workload. The setting is in ManageWorkspaces CR. Below is the sample. apiVersion : apps.mas.ibm.com/v1 kind : ManageWorkspace ... spec : settings : deployment : serverBundles : - bundleType : mea isDefault : false isMobileTarget : false isUserSyncTarget : true name : mea replica : 1 routeSubDomain : all - bundleType : cron isDefault : false isMobileTarget : false isUserSyncTarget : false name : cron replica : 1 ...","title":"App Server"},{"location":"mas/manage/bestpractice/#load-balancer","text":"Lab test shows roundrobin has more stable and better performance than leastconn policy which is the default. Follow this link to update load balance policy.","title":"Load-Balancer"},{"location":"mas/manage/bestpractice/#manage-pod-functionality","text":"Follow this page to understand the manage pod functionality.","title":"Manage Pod Functionality"},{"location":"mas/manage/bestpractice/#ltpa-timeout","text":"Using IBM Maximo Application Suite (MAS), Manage users will receive an error message saying to reload the application after 2 hours, even while actively working. This 2-hour timeout default is when the LTPA token in Manage expires, and is redirecting the user back to the login page for MAS. Follow Updating LTPA timeout in Manage to increase the default value.","title":"LTPA timeout"},{"location":"mas/manage/bestpractice/#websphere-liberty","text":"Due to the architecture change, Maximo 8.x (MAS Manage app) is deployed on WebSphere Liberty Base 21.0.0.5 with OpenJ9. As of WebSphere Liberty 18.0.0.1, the thread growth algorithm is enhanced to grow thread capacity more aggressively so as to react more rapidly to peak loads. For many environments, this autonomic tuning provided by the Open Liberty thread pool works well with no configuration or tuning by the operator. If necessary, you can adjust coreThreads and maxThreads value. Follow this page for tuning liberty","title":"WebSphere Liberty"},{"location":"mas/manage/bestpractice/#configure-jvm-options-in-manage-app","text":"Follow this page to configure JVM options","title":"Configure JVM options in Manage app"},{"location":"mas/manage/bestpractice/#db","text":"","title":"DB"},{"location":"mas/manage/bestpractice/#disk","text":"disk performance is critial for db performance. Recommend a storage or disk with disk throughput > 200 MB/s storage class: recommend a storage with 100+ IOPS","title":"Disk"},{"location":"mas/manage/bestpractice/#db-db2db2wh","text":"DB2 Tuning in Maximo 7.6.x Best practice is applicable. Highlights: increase maxsequence cache to 50 run runstats and/or reorg to update index periodically separate system storage, user storage, backup storage, transaction logs storage, temporary tablespace storage on as different disks if possible. Use DB2 Performance Diagnosis to troubleshoot and tuning the db and SQL. Manage requires row-organized tables. Check db2w db setting (by default it uses column based) and update the setting by db2 update db cfg using DFT_TABLE_ORG ROW Manage does NOT support MMP or tale partition in the current version, but consider to archive records over 1-year old. Optim is the one of the tools can be used for archiving. see this guide and this video for details. Increase the concurrently running statements allowed for a DB2 application. This issue occcurs when loading a large amount of data via MIF or api call. See this link for the tuning. storageclass for ibm cloud: performance(Custom) block storage with 100+ IOPS for data storage, block gold for system and block silver for backup for aws cloud: if using EFS for db, consider Provisioned mode to have a constant throughput. For more disk options, see details in this page","title":"DB - DB2/DB2wh"},{"location":"mas/manage/bestpractice/#db-oracle","text":"Maximo 7.6.x Best practice is applicable","title":"DB - Oracle"},{"location":"mas/manage/bestpractice/#db-mssql","text":"Maximo 7.6.x Best practice is applicable additional settings for MSSQL Server 2019 compatibility level: if maximo db is upgraded from the old version and the performance degradtation is observed after the upgrade, consider to set compatibility level to the old version to keep the execution plan same. isolation level: ALTER DATABASE < DB NAME > SET ALLOW_SNAPSHOT_ISOLATION ON ALTER DATABASE < DB NAME > SET READ_COMMITTED_SNAPSHOT ON","title":"DB - MSSQL"},{"location":"mas/mongodb/bestpractice/","text":"MongoDB \uf0c1 MongoDB Troubleshoot: \uf0c1 mongostat: mongostat --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 mongotop: mongotop --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 long connection over 3 seconds: db.currentOp({\"active\" : true,\"secs_running\" : { \"$gt\" : 3 },\"ns\" : /^msg/}) kill long running connection: db.killOp(\"opid\") locking: db.serverStatus().globalLock mem: db.serverStatus().mem wiredTiger cache: db.serverStatus().wiredTiger.cache concurrent: db.serverStatus().connections","title":"MongoDB"},{"location":"mas/mongodb/bestpractice/#mongodb","text":"","title":"MongoDB"},{"location":"mas/mongodb/bestpractice/#mongodb-troubleshoot","text":"mongostat: mongostat --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 mongotop: mongotop --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 long connection over 3 seconds: db.currentOp({\"active\" : true,\"secs_running\" : { \"$gt\" : 3 },\"ns\" : /^msg/}) kill long running connection: db.killOp(\"opid\") locking: db.serverStatus().globalLock mem: db.serverStatus().mem wiredTiger cache: db.serverStatus().wiredTiger.cache concurrent: db.serverStatus().connections","title":"MongoDB Troubleshoot:"},{"location":"mas/monitoring/guidance/","text":"Monitoring \uf0c1 Monitoring your OpenShift clusters is critical for the environment health, the quality of services. It helps ensure that all deployed workloads are running smoothly and that the environment is properly scoped. OpenShift Monitoring Service (Promethus/Grafana) \uf0c1 OpenShift Container Platform includes a pre-installed monitoring stack that is based on the Prometheus/Grafana. MAS also provides app-level promethus metrics and a set of Grafana dashboards for application health. More installation, configuration details can be found in IBM MAS Monitoring Best practice for OpenShift Monitoring Service enable User Workload: enableUserWorkload: false consider to increase the promethus retention policy whose default value is 24h and add persistent volumes consider to change Alert Manager's storage class and size Below is the sample for configmap cluster-monitoring-config apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : | enableUserWorkload: true prometheusK8s: retention: 90d volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: cpu: 200m storage: 300Gi memory: 2Gi limits: cpu: 2 memory: 4Gi alertmanagerMain: volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: storage: 20Gi Note Except OpenShift Monitoring Service (Promethus/Grafana), there are other paid solutions like IBM Instana , New Relic , Data Dog that also support OCP. If the cluster is cloud based, consider to use cloud provider's monitoring tool for additional info like network, disk, managed services. e.g. AWS CloudWatch, IBM Log Analysis...","title":"Monitoring"},{"location":"mas/monitoring/guidance/#monitoring","text":"Monitoring your OpenShift clusters is critical for the environment health, the quality of services. It helps ensure that all deployed workloads are running smoothly and that the environment is properly scoped.","title":"Monitoring"},{"location":"mas/monitoring/guidance/#openshift-monitoring-service-promethusgrafana","text":"OpenShift Container Platform includes a pre-installed monitoring stack that is based on the Prometheus/Grafana. MAS also provides app-level promethus metrics and a set of Grafana dashboards for application health. More installation, configuration details can be found in IBM MAS Monitoring Best practice for OpenShift Monitoring Service enable User Workload: enableUserWorkload: false consider to increase the promethus retention policy whose default value is 24h and add persistent volumes consider to change Alert Manager's storage class and size Below is the sample for configmap cluster-monitoring-config apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : | enableUserWorkload: true prometheusK8s: retention: 90d volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: cpu: 200m storage: 300Gi memory: 2Gi limits: cpu: 2 memory: 4Gi alertmanagerMain: volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: storage: 20Gi Note Except OpenShift Monitoring Service (Promethus/Grafana), there are other paid solutions like IBM Instana , New Relic , Data Dog that also support OCP. If the cluster is cloud based, consider to use cloud provider's monitoring tool for additional info like network, disk, managed services. e.g. AWS CloudWatch, IBM Log Analysis...","title":"OpenShift Monitoring Service (Promethus/Grafana)"},{"location":"mas/ocp/bestpractice/","text":"OpenShift Container Platform \uf0c1 Cluster Insights Advisor \uf0c1 Highly recommend to use OpenShift cluster Insights Advisor that to check any issue related to the current version, nodes and mis-configurations. It is the first step for the problem diagnosis. Steps: Login on OpenShift Console Go to Administration -> Cluster Settings Click OpenShift Cluster Manager in Subscription section. It redirects the url to RedHat Hybrid Cloud Console Click Insights Advisor PID limit for docker \uf0c1 This settings how many processes can be run within one single container. If it is too small, it can cause folk bomb issue. E.g. db2w instance may be unavailable when there are thousands of connections/agents upcoming or Openshift Container Storage not behaving well with a large amount of PVCs. OOB value for OCP platforms: Platform Version Default Value IBM ROKS (4.8) 231239 AWS ROSA 1024 (Not Specified) Azure Self-Managed OCP 1024 Steps to check or update PID limit: $ oc debug node/$NODE_NAME $ chroot /host $ cat /etc/crio/crio.conf # add / modify the line \"pids_limit = <new value>\" # run belows commands to reboot services and worker nodes $ systemctl daemon-reload $ systemctl restart crio $ shutdown -r now HAProxy Router \uf0c1 Ingress Controller \uf0c1 Openshift HAProxy supports up to 20k connection per pod. Consider to scale up ingress pod for any app (like IoT) with a high volume connection workload. Scale up ingress controller command: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge Max Connection \uf0c1 One of the most important tunable parameters for HAProxy scalability is the maxconn parameter. The router can handle a maximum number of 20000 concurrent connections by using oc adm router --max-connections=xxxxx . This parameter will be impacted by node settings sysctl fs.nr_open and sysctl fs.file-max . HAproxy will not start if maxconn is high, but node setting is low. Note: OpenShift Container Platform no longer supports modifying Ingress Controller deployments by setting environment variables such as ROUTER_THREADS, ROUTER_DEFAULT_TUNNEL_TIMEOUT, ROUTER_DEFAULT_CLIENT_TIMEOUT, ROUTER_DEFAULT_SERVER_TIMEOUT, and RELOAD_INTERVAL. You can modify the Ingress Controller deployment, but if the Ingress Operator is enabled, the configuration is overwritten. Load Balance Algorithm \uf0c1 There are 3 load-balancing algorithms: source, roundrobin, and leastconn. leastconn is the default. Set up annotations for each route to change the default algorithm. e.g. haproxy.router.openshift.io/balance=roundrobin","title":"OpenShift Container Platform"},{"location":"mas/ocp/bestpractice/#openshift-container-platform","text":"","title":"OpenShift Container Platform"},{"location":"mas/ocp/bestpractice/#cluster-insights-advisor","text":"Highly recommend to use OpenShift cluster Insights Advisor that to check any issue related to the current version, nodes and mis-configurations. It is the first step for the problem diagnosis. Steps: Login on OpenShift Console Go to Administration -> Cluster Settings Click OpenShift Cluster Manager in Subscription section. It redirects the url to RedHat Hybrid Cloud Console Click Insights Advisor","title":"Cluster Insights Advisor"},{"location":"mas/ocp/bestpractice/#pid-limit-for-docker","text":"This settings how many processes can be run within one single container. If it is too small, it can cause folk bomb issue. E.g. db2w instance may be unavailable when there are thousands of connections/agents upcoming or Openshift Container Storage not behaving well with a large amount of PVCs. OOB value for OCP platforms: Platform Version Default Value IBM ROKS (4.8) 231239 AWS ROSA 1024 (Not Specified) Azure Self-Managed OCP 1024 Steps to check or update PID limit: $ oc debug node/$NODE_NAME $ chroot /host $ cat /etc/crio/crio.conf # add / modify the line \"pids_limit = <new value>\" # run belows commands to reboot services and worker nodes $ systemctl daemon-reload $ systemctl restart crio $ shutdown -r now","title":"PID limit for docker"},{"location":"mas/ocp/bestpractice/#haproxy-router","text":"","title":"HAProxy Router"},{"location":"mas/ocp/bestpractice/#ingress-controller","text":"Openshift HAProxy supports up to 20k connection per pod. Consider to scale up ingress pod for any app (like IoT) with a high volume connection workload. Scale up ingress controller command: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge","title":"Ingress Controller"},{"location":"mas/ocp/bestpractice/#max-connection","text":"One of the most important tunable parameters for HAProxy scalability is the maxconn parameter. The router can handle a maximum number of 20000 concurrent connections by using oc adm router --max-connections=xxxxx . This parameter will be impacted by node settings sysctl fs.nr_open and sysctl fs.file-max . HAproxy will not start if maxconn is high, but node setting is low. Note: OpenShift Container Platform no longer supports modifying Ingress Controller deployments by setting environment variables such as ROUTER_THREADS, ROUTER_DEFAULT_TUNNEL_TIMEOUT, ROUTER_DEFAULT_CLIENT_TIMEOUT, ROUTER_DEFAULT_SERVER_TIMEOUT, and RELOAD_INTERVAL. You can modify the Ingress Controller deployment, but if the Ingress Operator is enabled, the configuration is overwritten.","title":"Max Connection"},{"location":"mas/ocp/bestpractice/#load-balance-algorithm","text":"There are 3 load-balancing algorithms: source, roundrobin, and leastconn. leastconn is the default. Set up annotations for each route to change the default algorithm. e.g. haproxy.router.openshift.io/balance=roundrobin","title":"Load Balance Algorithm"},{"location":"mas/sizing/guidance/","text":"Sizing Guidance \uf0c1 The sizing number in this page is based on a standard workload. Used as reference only. Sizing Calculation Sheet \uf0c1 Use Sizing Calculation Sheet for MAS sizing. Factors that impact the sizing consideration \uf0c1 storage operator: e.g. ocs, odf... cp4d services: e.g. db2w, watson studio... mongodb service kafka service OCS (OpenShift Container Storage) \uf0c1 If using OCS to manage the storage class, OCS service itself requires minimum 3 nodes with 14 core / 32G (Note: this is the total request amount, not per node). ODF (OpenShift Data Foundation) \uf0c1 3 OCP nodes will run ODF services. (NOTE: OCP clusters often contain additional OCP worker nodes which do not run ODF services.) Each OCP node running ODF services has:16 core / 64 GB memory CP4D/DB2W Minimum Resource Requirement \uf0c1 When running CP4D/DB2W on OpenShift's worknode, each instance requires at least 6.1 core and 18G ram . Note: an instance pod cannot be scheduled if the node's (total capacity - total limit) is less than 6.1 core or 18G ram, a dedicated worker node or external db is recommended. db2 operator is an alternative. MAS Resource Statistics \uf0c1 Namespace CPU Request CPU Limits Memory Request(GB) Memory Limits(GB) mas-masinst1-core 0.71 18.95 6.18 32.50 mas-masinst1-manage 2.5 11.1 4.04 17.00 mas-innovation-monitor 3.5 19.25 12.84 32.57 mas-innovation-iot 19.66 214.649 57.08 269.00 mongo 3 9 2.24 6.98 ibm-sls 0.12 2.6 0.56 4.50 kafka 9.2 31 30.38 30.38 ibm-common-services 0.81 1.92 1.30 2.73 OpenShift 3.2 28 Subtotal 42.7(core) 308.469(core) 142.62(GB) 395.66 (GB) Additional cost - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ocs* 14 32 14 32 cp4d (with 2 db2w instances)* 31.59 40.7 235.39 249.70 each additional manage pod* 1 6 2 10","title":"Sizing Guidance"},{"location":"mas/sizing/guidance/#sizing-guidance","text":"The sizing number in this page is based on a standard workload. Used as reference only.","title":"Sizing Guidance"},{"location":"mas/sizing/guidance/#sizing-calculation-sheet","text":"Use Sizing Calculation Sheet for MAS sizing.","title":"Sizing Calculation Sheet"},{"location":"mas/sizing/guidance/#factors-that-impact-the-sizing-consideration","text":"storage operator: e.g. ocs, odf... cp4d services: e.g. db2w, watson studio... mongodb service kafka service","title":"Factors that impact the sizing consideration"},{"location":"mas/sizing/guidance/#ocs-openshift-container-storage","text":"If using OCS to manage the storage class, OCS service itself requires minimum 3 nodes with 14 core / 32G (Note: this is the total request amount, not per node).","title":"OCS (OpenShift Container Storage)"},{"location":"mas/sizing/guidance/#odf-openshift-data-foundation","text":"3 OCP nodes will run ODF services. (NOTE: OCP clusters often contain additional OCP worker nodes which do not run ODF services.) Each OCP node running ODF services has:16 core / 64 GB memory","title":"ODF (OpenShift Data Foundation)"},{"location":"mas/sizing/guidance/#cp4ddb2w-minimum-resource-requirement","text":"When running CP4D/DB2W on OpenShift's worknode, each instance requires at least 6.1 core and 18G ram . Note: an instance pod cannot be scheduled if the node's (total capacity - total limit) is less than 6.1 core or 18G ram, a dedicated worker node or external db is recommended. db2 operator is an alternative.","title":"CP4D/DB2W Minimum Resource Requirement"},{"location":"mas/sizing/guidance/#mas-resource-statistics","text":"Namespace CPU Request CPU Limits Memory Request(GB) Memory Limits(GB) mas-masinst1-core 0.71 18.95 6.18 32.50 mas-masinst1-manage 2.5 11.1 4.04 17.00 mas-innovation-monitor 3.5 19.25 12.84 32.57 mas-innovation-iot 19.66 214.649 57.08 269.00 mongo 3 9 2.24 6.98 ibm-sls 0.12 2.6 0.56 4.50 kafka 9.2 31 30.38 30.38 ibm-common-services 0.81 1.92 1.30 2.73 OpenShift 3.2 28 Subtotal 42.7(core) 308.469(core) 142.62(GB) 395.66 (GB) Additional cost - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ocs* 14 32 14 32 cp4d (with 2 db2w instances)* 31.59 40.7 235.39 249.70 each additional manage pod* 1 6 2 10","title":"MAS Resource Statistics"},{"location":"maximo-7/bestpractice/","text":"Info The next version of Maximo 7 is MAS Manage 8.x. This page includes the best practice documents for Maximo EAM v7 only. The DB section in each best practice is still applicable to MAS Manage. Best Practices for Maximo 7.x \uf0c1 Maximo Best Practices for System Performance 7.6.x (Version 1.3) Maximo Anywhere 7.6.2 Best Practices (2018 Edition) Maximo Performance Tuning Tips: Additional tips on top of Best Practices for System Performance Maximo Best Practices for System Performance 7.5.x (Version 2.1) Improving Start Center performance in Maximo System Properties to Monitor and Troubleshoot Performance","title":"Best Practice"},{"location":"maximo-7/bestpractice/#best-practices-for-maximo-7x","text":"Maximo Best Practices for System Performance 7.6.x (Version 1.3) Maximo Anywhere 7.6.2 Best Practices (2018 Edition) Maximo Performance Tuning Tips: Additional tips on top of Best Practices for System Performance Maximo Best Practices for System Performance 7.5.x (Version 2.1) Improving Start Center performance in Maximo System Properties to Monitor and Troubleshoot Performance","title":"Best Practices for Maximo 7.x"},{"location":"pd/checklist/","text":"Performance Diagnosis \uf0c1 Info A monitoring system is strongly recommended to track the environment health and the quality of services. Diagnostic Utility \uf0c1 Scope Name Used for OCP OpenShift Monitoring Service OpenShift Cluster and MAS DB2 IBM DSM DB2 Historical and Realtime Troubleshooting DB2 db2top DB2 Realtime Troubleshooting Oracle AWR, StatsPack Historical Troubleshooting JVM IBM Support Assistant Heap Dump and GC Log Analysis JVM MAT JVM Dump Analysis Maximo PerfMon - Maximo UI Activity Tracing - Note : Enabling PerfMon may significantly degrade server performance. - Recommend for a single user with Dev/Test env only MongoDB mongotop MongoDB Realtime Troubleshooting HAR HTTP Archive Viewer HAR Analysis - for web page and client side (browser) performance SQL Poor SQL Online SQL Formatter SQL Squirrl Universal SQL Client SSL SSL Shopper Online certificate decode tool Factors in system performance \uf0c1 System performance depends on more than the applications and the database. The network architecture affects performance. Application server configuration can hurt or improve performance. The way that you deploy Maximo across servers affects the way the products perform. Many other factors come into play in providing the end-user experience of system performance. Subsequent sections in this paper address the following topics: System architecture setup including OCP, Instance Type, Storage App and DB server configuration Network issues Bandwidth Load balancing Database tuning SQL tuning Scheduled tasks (cron tasks) Reporting Integration with other systems using the integration framework Troubleshooting Performance Check List \uf0c1 check node status. e.g. any NOT Ready worker nodes if there is any pod or node cpu, memeory usage approaching to the limit? if there is any pod restarted many time recently? if there is any JVM Heapdump dump? if there is any JVM Hung Thread if there is any node or pod with a high system or IO wait (20%)? if there is any node memory, disk or pid pressure? if the response time is high (over 2 sec)? if any long running (over 2 sec) or high cpu cost query? if there is network bottleneck (e.g. load-balancer) is app server or db server busy? if app server is busy check the request, limit value for cpu, memory should replic memebers be increased? if db server is busy check cpu, memory, disk current usage and limit value check any utility in the background. e.g. backup check db lock check if there is any high cost query check disk performance","title":"Diagnosis Check List"},{"location":"pd/checklist/#performance-diagnosis","text":"Info A monitoring system is strongly recommended to track the environment health and the quality of services.","title":"Performance Diagnosis"},{"location":"pd/checklist/#diagnostic-utility","text":"Scope Name Used for OCP OpenShift Monitoring Service OpenShift Cluster and MAS DB2 IBM DSM DB2 Historical and Realtime Troubleshooting DB2 db2top DB2 Realtime Troubleshooting Oracle AWR, StatsPack Historical Troubleshooting JVM IBM Support Assistant Heap Dump and GC Log Analysis JVM MAT JVM Dump Analysis Maximo PerfMon - Maximo UI Activity Tracing - Note : Enabling PerfMon may significantly degrade server performance. - Recommend for a single user with Dev/Test env only MongoDB mongotop MongoDB Realtime Troubleshooting HAR HTTP Archive Viewer HAR Analysis - for web page and client side (browser) performance SQL Poor SQL Online SQL Formatter SQL Squirrl Universal SQL Client SSL SSL Shopper Online certificate decode tool","title":"Diagnostic Utility"},{"location":"pd/checklist/#factors-in-system-performance","text":"System performance depends on more than the applications and the database. The network architecture affects performance. Application server configuration can hurt or improve performance. The way that you deploy Maximo across servers affects the way the products perform. Many other factors come into play in providing the end-user experience of system performance. Subsequent sections in this paper address the following topics: System architecture setup including OCP, Instance Type, Storage App and DB server configuration Network issues Bandwidth Load balancing Database tuning SQL tuning Scheduled tasks (cron tasks) Reporting Integration with other systems using the integration framework Troubleshooting","title":"Factors in system performance"},{"location":"pd/checklist/#performance-check-list","text":"check node status. e.g. any NOT Ready worker nodes if there is any pod or node cpu, memeory usage approaching to the limit? if there is any pod restarted many time recently? if there is any JVM Heapdump dump? if there is any JVM Hung Thread if there is any node or pod with a high system or IO wait (20%)? if there is any node memory, disk or pid pressure? if the response time is high (over 2 sec)? if any long running (over 2 sec) or high cpu cost query? if there is network bottleneck (e.g. load-balancer) is app server or db server busy? if app server is busy check the request, limit value for cpu, memory should replic memebers be increased? if db server is busy check cpu, memory, disk current usage and limit value check any utility in the background. e.g. backup check db lock check if there is any high cost query check disk performance","title":"Performance Check List"},{"location":"pd/db2-performance-diagnosis/","text":"DB2TOP \uf0c1 db2top can be used for a real-time diagnosis. Command: db2top -db <dbname> press h : help screen press I : reset the interval time (default is 2 seconds) press m : memory screen press B : bottleneck screen press b : bufferpool screen press T : Table screen press U : locks screen press u : utility screen to check if runstat is running press D : Dynamic SQL screen Catch High CPU SQL in Dynamic SQL screen, do: Press z and 5 to sort by cpu usage Copy SQL Hashcode Press L and Paste SQL Hashcode Notes: Be caution to take any snapshot. See more details on User Manual Diagnosis Commands \uf0c1 list memory allocation: db2mtrk -i -d \u2013v list long run query: SELECT ELAPSED_TIME_MIN,SUBSTR(AUTHID,1,10) AS AUTH_ID, AGENT_ID,APPL_STATUS,SUBSTR(STMT_TEXT,1,20) AS SQL_TEXT FROM SYSIBMADM.LONG_RUNNING_SQL WHERE ELAPSED_TIME_MIN > 0 ORDER BY ELAPSED_TIME_MIN DESC; list backup/restore status: db2pd -barstats -d <dbname> list most active tables: SELECT SUBSTR(TABSCHEMA,1,10) AS SCHEMA,SUBSTR(TABNAME,1,20) AS NAME,TABLE_SCANS,ROWS_READ,ROWS_INSERTED,ROWS_DELETED FROM TABLE(MON_GET_TABLE('','',-1)) ORDER BY ROWS_READ DESC FETCH FIRST 5 ROWS ONLY list most active indexes: SELECT SUBSTR(TABSCHEMA,1,10) AS SCHEMA,SUBSTR(TABNAME,1,20) AS NAME,IID,NLEAF, NLEVELS,INDEX_SCANS,KEY_UPDATES,BOUNDARY_LEAF_NODE_SPLITS + NONBOUNDARY_LEAF_NODE_SPLITS AS PAGE_SPLITS FROM TABLE(MON_GET_INDEX('','',-1)) ORDER BY INDEX_SCANS DESC FETCH FIRST 5 ROWS ONLY list db2 advise for the statement: db2advis -database bludb -s \"select * from maximo.ahfactorhistory where ahdriverhistoryid = 123 for read only\" -n MAXIMO -q MAXIMO list the query execution plan: db2expln -database bludb -schema MAXIMO -package % -statement \"select * from maximo.ahfactorhistory where ahdriverhistoryid = 123 for read only\" -terminal -graph > query1_access_plan.txt list all indexes for a specific table: select * from syscat.indexes i where TABNAME ='ITEMSTRUCT' list insert/update/delete/tablescan stats for a specific table: SELECT rows_read,rows_inserted,rows_updated,rows_deleted,table_scans FROM TABLE(MON_GET_TABLE('MAXIMO','ASSET',-2)) list insert/update/delete/tablescan stats for all tables: SELECT SUBSTR(TABSCHEMA,1,10) AS SCHEMA,SUBSTR(TABNAME,1,20) AS NAME,TABLE_SCANS,ROWS_READ,ROWS_INSERTED,ROWS_DELETED FROM TABLE(MON_GET_TABLE('','',-1)) ORDER BY ROWS_READ DESC FETCH FIRST 5 ROWS ONLY\" list error message: db2 ? <sqlerror> db2pd : monitor and troubleshoot DB2 database command db2diag : db2diag logs analysis tool command db2set : db2 global settings db2 get dbm cfg : db2 database manager configuration db2 get db cfg : db2 database configuration IBM Data Server Manager (IBM DSM) \uf0c1 IBM DSM is useful to do both real-time/ historical data diagnosis, find out the expensive sql query, justify cpu spent on sql execution or other e.g. sorting, parsing, fetching, io and so on. It requires pre-configuration. A high-level set up: Download the latest version of Data Server Manager from IBM developerWorks or IBM Passport Advantage Online , then extract to /opt/ibm/dsm run setup.sh to set up and create admin user run start.sh to start the server, url is http://hostname:11080/console after log on the console, select a time period e.g. peak time, then generate report.","title":"DB2 Performance Diagnosis"},{"location":"pd/db2-performance-diagnosis/#db2top","text":"db2top can be used for a real-time diagnosis. Command: db2top -db <dbname> press h : help screen press I : reset the interval time (default is 2 seconds) press m : memory screen press B : bottleneck screen press b : bufferpool screen press T : Table screen press U : locks screen press u : utility screen to check if runstat is running press D : Dynamic SQL screen Catch High CPU SQL in Dynamic SQL screen, do: Press z and 5 to sort by cpu usage Copy SQL Hashcode Press L and Paste SQL Hashcode Notes: Be caution to take any snapshot. See more details on User Manual","title":"DB2TOP"},{"location":"pd/db2-performance-diagnosis/#diagnosis-commands","text":"list memory allocation: db2mtrk -i -d \u2013v list long run query: SELECT ELAPSED_TIME_MIN,SUBSTR(AUTHID,1,10) AS AUTH_ID, AGENT_ID,APPL_STATUS,SUBSTR(STMT_TEXT,1,20) AS SQL_TEXT FROM SYSIBMADM.LONG_RUNNING_SQL WHERE ELAPSED_TIME_MIN > 0 ORDER BY ELAPSED_TIME_MIN DESC; list backup/restore status: db2pd -barstats -d <dbname> list most active tables: SELECT SUBSTR(TABSCHEMA,1,10) AS SCHEMA,SUBSTR(TABNAME,1,20) AS NAME,TABLE_SCANS,ROWS_READ,ROWS_INSERTED,ROWS_DELETED FROM TABLE(MON_GET_TABLE('','',-1)) ORDER BY ROWS_READ DESC FETCH FIRST 5 ROWS ONLY list most active indexes: SELECT SUBSTR(TABSCHEMA,1,10) AS SCHEMA,SUBSTR(TABNAME,1,20) AS NAME,IID,NLEAF, NLEVELS,INDEX_SCANS,KEY_UPDATES,BOUNDARY_LEAF_NODE_SPLITS + NONBOUNDARY_LEAF_NODE_SPLITS AS PAGE_SPLITS FROM TABLE(MON_GET_INDEX('','',-1)) ORDER BY INDEX_SCANS DESC FETCH FIRST 5 ROWS ONLY list db2 advise for the statement: db2advis -database bludb -s \"select * from maximo.ahfactorhistory where ahdriverhistoryid = 123 for read only\" -n MAXIMO -q MAXIMO list the query execution plan: db2expln -database bludb -schema MAXIMO -package % -statement \"select * from maximo.ahfactorhistory where ahdriverhistoryid = 123 for read only\" -terminal -graph > query1_access_plan.txt list all indexes for a specific table: select * from syscat.indexes i where TABNAME ='ITEMSTRUCT' list insert/update/delete/tablescan stats for a specific table: SELECT rows_read,rows_inserted,rows_updated,rows_deleted,table_scans FROM TABLE(MON_GET_TABLE('MAXIMO','ASSET',-2)) list insert/update/delete/tablescan stats for all tables: SELECT SUBSTR(TABSCHEMA,1,10) AS SCHEMA,SUBSTR(TABNAME,1,20) AS NAME,TABLE_SCANS,ROWS_READ,ROWS_INSERTED,ROWS_DELETED FROM TABLE(MON_GET_TABLE('','',-1)) ORDER BY ROWS_READ DESC FETCH FIRST 5 ROWS ONLY\" list error message: db2 ? <sqlerror> db2pd : monitor and troubleshoot DB2 database command db2diag : db2diag logs analysis tool command db2set : db2 global settings db2 get dbm cfg : db2 database manager configuration db2 get db cfg : db2 database configuration","title":"Diagnosis Commands"},{"location":"pd/db2-performance-diagnosis/#ibm-data-server-manager-ibm-dsm","text":"IBM DSM is useful to do both real-time/ historical data diagnosis, find out the expensive sql query, justify cpu spent on sql execution or other e.g. sorting, parsing, fetching, io and so on. It requires pre-configuration. A high-level set up: Download the latest version of Data Server Manager from IBM developerWorks or IBM Passport Advantage Online , then extract to /opt/ibm/dsm run setup.sh to set up and create admin user run start.sh to start the server, url is http://hostname:11080/console after log on the console, select a time period e.g. peak time, then generate report.","title":"IBM Data Server Manager (IBM DSM)"},{"location":"pd/ptbp/","text":"Performance Test Best Practice \uf0c1 There are 3 major performance tools used by the lab for performance test. Below are the best practice for each tool. Rational Performance Tester \uf0c1 Performance Test Best Practices for Rational Performance Tester LoadRunner \uf0c1 Performance Test Best Practices for LoadRunner JMeter \uf0c1 JMeter Best Practice","title":"Performance Test Best Practice"},{"location":"pd/ptbp/#performance-test-best-practice","text":"There are 3 major performance tools used by the lab for performance test. Below are the best practice for each tool.","title":"Performance Test Best Practice"},{"location":"pd/ptbp/#rational-performance-tester","text":"Performance Test Best Practices for Rational Performance Tester","title":"Rational Performance Tester"},{"location":"pd/ptbp/#loadrunner","text":"Performance Test Best Practices for LoadRunner","title":"LoadRunner"},{"location":"pd/ptbp/#jmeter","text":"JMeter Best Practice","title":"JMeter"}]}