{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MAS Performance Wiki \uf0c1 Info This site will be upgraded periodically. More topics will be added soon. This site contains best practices for configuration, tuning, sizing and troubleshooting diagnosis tips for both Maximo Application Suite (MAS) and Maximo v7 .","title":"Home"},{"location":"#welcome-to-mas-performance-wiki","text":"Info This site will be upgraded periodically. More topics will be added soon. This site contains best practices for configuration, tuning, sizing and troubleshooting diagnosis tips for both Maximo Application Suite (MAS) and Maximo v7 .","title":"Welcome to MAS Performance Wiki"},{"location":"mas/aws/bestpractice/","text":"AWS \uf0c1 Instance Type \uf0c1 There are many instance type available in AWS. Based on the benchmark, recommend M5, M6 instances (e.g.M5.4xlarge) as master or worker nodes and P3, P4 as GPU nodes. Note Depending on the regions, some instances may not be available. Use AWS Pricing Calculator to check the instance availability and cost. g4dn can be used as GPU node for test/dev env, but not recommended for production env. If the application requires a good network performance, check Amazon EC2 instance network bandwidth site for more details. For production env, an instance with 10GB ethernet is recommended. Classic Load Balancer Idle Timeout \uf0c1 Each OCP cluster creates 1 class load balancer and 2 network load balancers in AWS. AWS classic load balancer has a default idle time 60 seconds. In some cases, this value is not enough for a long time transaction e.g. asset health check notebook. Consider to adjust this value to what the application needs e.g. 300 seconds. Also, monitoring classic load-balance performance is strongly recommend, particularly with IoT related app. (Note: Surge Queue Length's default/hard code limit is 1024 . when queue is fully, the tcp handshake will fail) Amazon DocumentDB \uf0c1 DocumentDB that is a fully managed MongoDB compatibility database. It can be used by both IBM Suite License Service (SLS) and MAS Core. However, there are functional differences between DocumentDB and MongoDB. Check this link for more details. Note: When using DocumentDB, it requires to set RetryWrite=false in SLS and Suite CRs. Amazon MSK \uf0c1 MAS supports MSK which is a fully managed apache Kafka service. Note monitor MSK performance via CloudWatch is strongly recommended. Key metrics include Disk usage by broker, CPU (User) usage by broker, Active Controller Count, Network RX packets by broker, Network TX packets by broker . define an appropriate config for Kafka, MSK and topics. e.g. retention.ms, retention.bytes, partitions and replics to support the workload. AWS Storage \uf0c1 EBS storages like gp2, gp3 is supported by OCP in AWS. Note: EBS storage is ReadWriteOnce. The volume can be mounted as read-write by a single node. io1 and io2 are SSD-based EBS that provides the higher performance. Check Amazon EBS volume types for extra info like throughput, tuning and cost. Below is a sample yaml to create io1 storageclass with 100 iopsPerGB . kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : io1 provisioner : kubernetes.io/aws-ebs parameters : encrypted : 'true' iopsPerGB : '100' type : io1 reclaimPolicy : Delete allowVolumeExpansion : true volumeBindingMode : Immediate EFS Storage can be used as ReadWriteMany storageclass. EFS has different metered throughput modes. Bursting Throughput mode is the default. It is inexpensive, but does NOT perform well if all burst credits are used. Monitor BurstCreditBalance metric in CloudWatch. Provisioned Throughput mode is relatively expensive. It can drive up to 3 GiBps for read operations and 1 GiBps for write operations per file system More info can be found at Amazon EFS performance Self-managed OCP vs AWS ROSA \uf0c1 A self-managed OCP Cluster can be created by the installer cli tool that supports both IPI and UPI mode. It requires self maintenance and upgrades. Alternatively, ROSA is a managed Red Hat OpenShift Service. Each ROSA cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and Amazon support.","title":"AWS"},{"location":"mas/aws/bestpractice/#aws","text":"","title":"AWS"},{"location":"mas/aws/bestpractice/#instance-type","text":"There are many instance type available in AWS. Based on the benchmark, recommend M5, M6 instances (e.g.M5.4xlarge) as master or worker nodes and P3, P4 as GPU nodes. Note Depending on the regions, some instances may not be available. Use AWS Pricing Calculator to check the instance availability and cost. g4dn can be used as GPU node for test/dev env, but not recommended for production env. If the application requires a good network performance, check Amazon EC2 instance network bandwidth site for more details. For production env, an instance with 10GB ethernet is recommended.","title":"Instance Type"},{"location":"mas/aws/bestpractice/#classic-load-balancer-idle-timeout","text":"Each OCP cluster creates 1 class load balancer and 2 network load balancers in AWS. AWS classic load balancer has a default idle time 60 seconds. In some cases, this value is not enough for a long time transaction e.g. asset health check notebook. Consider to adjust this value to what the application needs e.g. 300 seconds. Also, monitoring classic load-balance performance is strongly recommend, particularly with IoT related app. (Note: Surge Queue Length's default/hard code limit is 1024 . when queue is fully, the tcp handshake will fail)","title":"Classic Load Balancer Idle Timeout"},{"location":"mas/aws/bestpractice/#amazon-documentdb","text":"DocumentDB that is a fully managed MongoDB compatibility database. It can be used by both IBM Suite License Service (SLS) and MAS Core. However, there are functional differences between DocumentDB and MongoDB. Check this link for more details. Note: When using DocumentDB, it requires to set RetryWrite=false in SLS and Suite CRs.","title":"Amazon DocumentDB"},{"location":"mas/aws/bestpractice/#amazon-msk","text":"MAS supports MSK which is a fully managed apache Kafka service. Note monitor MSK performance via CloudWatch is strongly recommended. Key metrics include Disk usage by broker, CPU (User) usage by broker, Active Controller Count, Network RX packets by broker, Network TX packets by broker . define an appropriate config for Kafka, MSK and topics. e.g. retention.ms, retention.bytes, partitions and replics to support the workload.","title":"Amazon MSK"},{"location":"mas/aws/bestpractice/#aws-storage","text":"EBS storages like gp2, gp3 is supported by OCP in AWS. Note: EBS storage is ReadWriteOnce. The volume can be mounted as read-write by a single node. io1 and io2 are SSD-based EBS that provides the higher performance. Check Amazon EBS volume types for extra info like throughput, tuning and cost. Below is a sample yaml to create io1 storageclass with 100 iopsPerGB . kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : io1 provisioner : kubernetes.io/aws-ebs parameters : encrypted : 'true' iopsPerGB : '100' type : io1 reclaimPolicy : Delete allowVolumeExpansion : true volumeBindingMode : Immediate EFS Storage can be used as ReadWriteMany storageclass. EFS has different metered throughput modes. Bursting Throughput mode is the default. It is inexpensive, but does NOT perform well if all burst credits are used. Monitor BurstCreditBalance metric in CloudWatch. Provisioned Throughput mode is relatively expensive. It can drive up to 3 GiBps for read operations and 1 GiBps for write operations per file system More info can be found at Amazon EFS performance","title":"AWS Storage"},{"location":"mas/aws/bestpractice/#self-managed-ocp-vs-aws-rosa","text":"A self-managed OCP Cluster can be created by the installer cli tool that supports both IPI and UPI mode. It requires self maintenance and upgrades. Alternatively, ROSA is a managed Red Hat OpenShift Service. Each ROSA cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and Amazon support.","title":"Self-managed OCP vs AWS ROSA"},{"location":"mas/core/bestpractice/","text":"MAS Core \uf0c1 Workload Scaling ConfigMap \uf0c1 Since MAS 8.10, a configmap custom-wl-scaling is used to set the resource settings for cpu, memory and replic. Modify the default value or create a custom one if needed. Below is the sample of workloadScaling.yaml . coreidpcfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi slscfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi Tip high workload impact pods: core-ipd, core-api, license-mediator check MAS Pods Explained to understand the insight of each pod functionality check the health of mongodb and sls pods if any performance issue is detected","title":"MAS Core"},{"location":"mas/core/bestpractice/#mas-core","text":"","title":"MAS Core"},{"location":"mas/core/bestpractice/#workload-scaling-configmap","text":"Since MAS 8.10, a configmap custom-wl-scaling is used to set the resource settings for cpu, memory and replic. Modify the default value or create a custom one if needed. Below is the sample of workloadScaling.yaml . coreidpcfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi slscfg : replicas : 1 resources : requests : cpu : 10m memory : 64Mi limits : cpu : 200m memory : 512Mi Tip high workload impact pods: core-ipd, core-api, license-mediator check MAS Pods Explained to understand the insight of each pod functionality check the health of mongodb and sls pods if any performance issue is detected","title":"Workload Scaling ConfigMap"},{"location":"mas/ibmcloud/bestpractice/","text":"IBM Cloud \uf0c1 IBM Storage \uf0c1 IBM Cloud provides both block and file storages for OCP. Both storages support ReadWriteMany access. If the app requires a high-performance disks, consider to setup custom performance storageclass as blow: block storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : block100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-block reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer file storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : file100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-file reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer IBM External Load Balancer \uf0c1 If the built-in ingress load balancer in OCP is unable to scale to handle with \"large\" workloads (100K+ concurrent device connections), consider to provision an instance of IBM cloud NLB2.0 (IPVS/KeepAlived) load balancer. IBM ROKS \uf0c1 IBM ROKS is a managed Red Hat OpenShift Service in IBM Cloud. Each ROKS cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and IBM Cloud support.","title":"IBM Cloud"},{"location":"mas/ibmcloud/bestpractice/#ibm-cloud","text":"","title":"IBM Cloud"},{"location":"mas/ibmcloud/bestpractice/#ibm-storage","text":"IBM Cloud provides both block and file storages for OCP. Both storages support ReadWriteMany access. If the app requires a high-performance disks, consider to setup custom performance storageclass as blow: block storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : block100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-block reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer file storage sample yaml allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : file100p parameters : billingType : hourly classVersion : \"2\" fsType : ext4 sizeIOPSRange : |- [20-1999]Gi:[100-100] type : Performance provisioner : ibm.io/ibmc-file reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer","title":"IBM Storage"},{"location":"mas/ibmcloud/bestpractice/#ibm-external-load-balancer","text":"If the built-in ingress load balancer in OCP is unable to scale to handle with \"large\" workloads (100K+ concurrent device connections), consider to provision an instance of IBM cloud NLB2.0 (IPVS/KeepAlived) load balancer.","title":"IBM External Load Balancer"},{"location":"mas/ibmcloud/bestpractice/#ibm-roks","text":"IBM ROKS is a managed Red Hat OpenShift Service in IBM Cloud. Each ROKS cluster comes with a fully managed control plane and compute nodes. Installation, management, maintenance, and upgrades are performed by Red Hat site reliability engineers (SRE) with joint Red Hat and IBM Cloud support.","title":"IBM ROKS"},{"location":"mas/iot/bestpractice/","text":"MAS IoT \uf0c1 IoT Deployment \uf0c1 IoT CRD defines 3 default size deployments: dev, small, medium that controls the default settings for pod replics, cpu and memory. For production, medium is required. Sample yaml for medium deployment in IoT CR apiVersion : iot.ibm.com/v1 kind : IoT metadata : name : masinst1 namespace : mas-masinst1-iot spec : bindings : jdbc : system kafka : system mongo : system settings : deployment : size : medium If need to adjust the default setting for a deployment, go the iot-operator pod, then change the corresponding yaml files under /opt/ansible/roles/<ibm-iot-operator>/vars folder, e.g. /opt/ansible/roles/ibm-iot-actions/vars/size_medium.yml Connection and OpenShift Ingress Controllers \uf0c1 Openshift HAProxy supports 20k connection per pod. The total connection determinants how many end devices can connect to IoT MSProxy. By default, IBM ROKS deploys 3 router members that supports 3x20k = 60K connection By default, AWS ROSA deploys 2 router members that supports 2x20k = 40K connection use the below command to scale up to 3 router members: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge Kafka \uf0c1 IoT uses Kafka to process the messages. Follow the Kafka Configuration Reference to configure best value for Kafka/Topics retention.ms, retention.bytes, partitions, replics to support the workload. AWS MSK \uf0c1 configuration details can be found at https://docs.aws.amazon.com/msk/latest/developerguide/msk-default-configuration.html monitoring MSK is strongly recommended monitoring classic load-balance is strongly recommend Message Rate and Ethernet Network Bandwidth \uf0c1 Depends on the cloud providers, worker node instance has difference network bandwidth. It determinants how fast the end devices can send the request. Message rate is limited by the message size and the bandwidth of ethernet network. To achieve higher rates and/or larger messages it will require a 10GB ethernet. The network bandwidth also impacts the response latency. The higher bandwidth, the lower latency. Below deployment configurations are recommended as starting value with medium and large workload. MSProxy - 4 MSProxies with 1 CPU and 4GB MessageGateWay - 1 MGW 6 CPUs and 16GB along with 4 TcpIop threads","title":"MAS IoT"},{"location":"mas/iot/bestpractice/#mas-iot","text":"","title":"MAS IoT"},{"location":"mas/iot/bestpractice/#iot-deployment","text":"IoT CRD defines 3 default size deployments: dev, small, medium that controls the default settings for pod replics, cpu and memory. For production, medium is required. Sample yaml for medium deployment in IoT CR apiVersion : iot.ibm.com/v1 kind : IoT metadata : name : masinst1 namespace : mas-masinst1-iot spec : bindings : jdbc : system kafka : system mongo : system settings : deployment : size : medium If need to adjust the default setting for a deployment, go the iot-operator pod, then change the corresponding yaml files under /opt/ansible/roles/<ibm-iot-operator>/vars folder, e.g. /opt/ansible/roles/ibm-iot-actions/vars/size_medium.yml","title":"IoT Deployment"},{"location":"mas/iot/bestpractice/#connection-and-openshift-ingress-controllers","text":"Openshift HAProxy supports 20k connection per pod. The total connection determinants how many end devices can connect to IoT MSProxy. By default, IBM ROKS deploys 3 router members that supports 3x20k = 60K connection By default, AWS ROSA deploys 2 router members that supports 2x20k = 40K connection use the below command to scale up to 3 router members: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge","title":"Connection and OpenShift Ingress Controllers"},{"location":"mas/iot/bestpractice/#kafka","text":"IoT uses Kafka to process the messages. Follow the Kafka Configuration Reference to configure best value for Kafka/Topics retention.ms, retention.bytes, partitions, replics to support the workload.","title":"Kafka"},{"location":"mas/iot/bestpractice/#aws-msk","text":"configuration details can be found at https://docs.aws.amazon.com/msk/latest/developerguide/msk-default-configuration.html monitoring MSK is strongly recommended monitoring classic load-balance is strongly recommend","title":"AWS MSK"},{"location":"mas/iot/bestpractice/#message-rate-and-ethernet-network-bandwidth","text":"Depends on the cloud providers, worker node instance has difference network bandwidth. It determinants how fast the end devices can send the request. Message rate is limited by the message size and the bandwidth of ethernet network. To achieve higher rates and/or larger messages it will require a 10GB ethernet. The network bandwidth also impacts the response latency. The higher bandwidth, the lower latency. Below deployment configurations are recommended as starting value with medium and large workload. MSProxy - 4 MSProxies with 1 CPU and 4GB MessageGateWay - 1 MGW 6 CPUs and 16GB along with 4 TcpIop threads","title":"Message Rate and Ethernet Network Bandwidth"},{"location":"mas/manage/bestpractice/","text":"MAS Manage \uf0c1 App Server \uf0c1 MAS Manage has different bundle types e.g. All, UI, MEA, Report and CRON to configure app server. Adjust the resource settings like cpu, memory, replic to match the workload. The setting is in ManageWorkspaces CR. Below is the sample. apiVersion : apps.mas.ibm.com/v1 kind : ManageWorkspace ... spec : settings : deployment : serverBundles : - bundleType : mea isDefault : false isMobileTarget : false isUserSyncTarget : true name : mea replica : 1 routeSubDomain : all - bundleType : cron isDefault : false isMobileTarget : false isUserSyncTarget : false name : cron replica : 1 ... DB Server \uf0c1 Follow Maximo 7 Best Practice for db configuration and tuning. Manage Pod Functionality \uf0c1 Follow this page to understand the manage pod functionality. LTPA timeout \uf0c1 Using IBM Maximo Application Suite (MAS), Manage users will receive an error message saying to reload the application after 2 hours, even while actively working. This 2-hour timeout default is when the LTPA token in Manage expires, and is redirecting the user back to the login page for MAS. Follow Updating LTPA timeout in Manage to increase the default value. WebSphere Liberty \uf0c1 Due to the architecture change, Maximo 8.x (MAS Manage app) is deployed on WebSphere Liberty Base 21.0.0.5 with OpenJ9. As of WebSphere Liberty 18.0.0.1, the thread growth algorithm is enhanced to grow thread capacity more aggressively so as to react more rapidly to peak loads. For many environments, this autonomic tuning provided by the Open Liberty thread pool works well with no configuration or tuning by the operator. If necessary, you can adjust coreThreads and maxThreads value. Follow this page for tuning liberty Configure JVM options in Manage app \uf0c1 Follow this page to configure JVM options Fo","title":"MAS Manage"},{"location":"mas/manage/bestpractice/#mas-manage","text":"","title":"MAS Manage"},{"location":"mas/manage/bestpractice/#app-server","text":"MAS Manage has different bundle types e.g. All, UI, MEA, Report and CRON to configure app server. Adjust the resource settings like cpu, memory, replic to match the workload. The setting is in ManageWorkspaces CR. Below is the sample. apiVersion : apps.mas.ibm.com/v1 kind : ManageWorkspace ... spec : settings : deployment : serverBundles : - bundleType : mea isDefault : false isMobileTarget : false isUserSyncTarget : true name : mea replica : 1 routeSubDomain : all - bundleType : cron isDefault : false isMobileTarget : false isUserSyncTarget : false name : cron replica : 1 ...","title":"App Server"},{"location":"mas/manage/bestpractice/#db-server","text":"Follow Maximo 7 Best Practice for db configuration and tuning.","title":"DB Server"},{"location":"mas/manage/bestpractice/#manage-pod-functionality","text":"Follow this page to understand the manage pod functionality.","title":"Manage Pod Functionality"},{"location":"mas/manage/bestpractice/#ltpa-timeout","text":"Using IBM Maximo Application Suite (MAS), Manage users will receive an error message saying to reload the application after 2 hours, even while actively working. This 2-hour timeout default is when the LTPA token in Manage expires, and is redirecting the user back to the login page for MAS. Follow Updating LTPA timeout in Manage to increase the default value.","title":"LTPA timeout"},{"location":"mas/manage/bestpractice/#websphere-liberty","text":"Due to the architecture change, Maximo 8.x (MAS Manage app) is deployed on WebSphere Liberty Base 21.0.0.5 with OpenJ9. As of WebSphere Liberty 18.0.0.1, the thread growth algorithm is enhanced to grow thread capacity more aggressively so as to react more rapidly to peak loads. For many environments, this autonomic tuning provided by the Open Liberty thread pool works well with no configuration or tuning by the operator. If necessary, you can adjust coreThreads and maxThreads value. Follow this page for tuning liberty","title":"WebSphere Liberty"},{"location":"mas/manage/bestpractice/#configure-jvm-options-in-manage-app","text":"Follow this page to configure JVM options Fo","title":"Configure JVM options in Manage app"},{"location":"mas/mongodb/bestpractice/","text":"MongoDB \uf0c1 MongoDB Troubleshoot: \uf0c1 mongostat: mongostat --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 mongotop: mongotop --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 long connection over 3 seconds: db.currentOp({\"active\" : true,\"secs_running\" : { \"$gt\" : 3 },\"ns\" : /^msg/}) kill long running connection: db.killOp(\"opid\") locking: db.serverStatus().globalLock mem: db.serverStatus().mem wiredTiger cache: db.serverStatus().wiredTiger.cache concurrent: db.serverStatus().connections","title":"MongoDB"},{"location":"mas/mongodb/bestpractice/#mongodb","text":"","title":"MongoDB"},{"location":"mas/mongodb/bestpractice/#mongodb-troubleshoot","text":"mongostat: mongostat --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 mongotop: mongotop --username admin --password <password> --authenticationDatabase admin --ssl --sslAllowInvalidCertificates 2 long connection over 3 seconds: db.currentOp({\"active\" : true,\"secs_running\" : { \"$gt\" : 3 },\"ns\" : /^msg/}) kill long running connection: db.killOp(\"opid\") locking: db.serverStatus().globalLock mem: db.serverStatus().mem wiredTiger cache: db.serverStatus().wiredTiger.cache concurrent: db.serverStatus().connections","title":"MongoDB Troubleshoot:"},{"location":"mas/monitoring/guidance/","text":"Monitoring \uf0c1 Monitoring your OpenShift clusters is critical for the environment health, the quality of services. It helps ensure that all deployed workloads are running smoothly and that the environment is properly scoped. OpenShift Monitoring Service (Promethus/Grafana) \uf0c1 OpenShift Container Platform includes a pre-installed monitoring stack that is based on the Prometheus/Grafana. MAS also provides app-level promethus metrics and a set of Grafana dashboards for application health. Best practice for OpenShift Monitoring Service enable User Workload: enableUserWorkload: false consider to increase the promethus retention policy whose default value is 24h and add persistent volumes consider to change Alert Manager's storage class and size Below is the sample for configmap cluster-monitoring-config apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : | enableUserWorkload: true prometheusK8s: retention: 90d volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: cpu: 200m storage: 300Gi memory: 2Gi limits: cpu: 2 memory: 4Gi alertmanagerMain: volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: storage: 20Gi Note Except OpenShift Monitoring Service (Promethus/Grafana), there are other paid solutions like IBM Instana , New Relic , Data Dog that also support OCP. If the cluster is cloud based, consider to use cloud provider's monitoring tool for additional info like network, disk, managed services. e.g. AWS CloudWatch, IBM Log Analysis...","title":"Monitoring"},{"location":"mas/monitoring/guidance/#monitoring","text":"Monitoring your OpenShift clusters is critical for the environment health, the quality of services. It helps ensure that all deployed workloads are running smoothly and that the environment is properly scoped.","title":"Monitoring"},{"location":"mas/monitoring/guidance/#openshift-monitoring-service-promethusgrafana","text":"OpenShift Container Platform includes a pre-installed monitoring stack that is based on the Prometheus/Grafana. MAS also provides app-level promethus metrics and a set of Grafana dashboards for application health. Best practice for OpenShift Monitoring Service enable User Workload: enableUserWorkload: false consider to increase the promethus retention policy whose default value is 24h and add persistent volumes consider to change Alert Manager's storage class and size Below is the sample for configmap cluster-monitoring-config apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : | enableUserWorkload: true prometheusK8s: retention: 90d volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: cpu: 200m storage: 300Gi memory: 2Gi limits: cpu: 2 memory: 4Gi alertmanagerMain: volumeClaimTemplate: spec: storageClassName: nfs-client resources: requests: storage: 20Gi Note Except OpenShift Monitoring Service (Promethus/Grafana), there are other paid solutions like IBM Instana , New Relic , Data Dog that also support OCP. If the cluster is cloud based, consider to use cloud provider's monitoring tool for additional info like network, disk, managed services. e.g. AWS CloudWatch, IBM Log Analysis...","title":"OpenShift Monitoring Service (Promethus/Grafana)"},{"location":"mas/ocp/bestpractice/","text":"OpenShift Container Platform \uf0c1 Cluster Insights Advisor \uf0c1 Highly recommend to use OpenShift cluster Insights Advisor that to check any issue related to the current version, nodes and mis-configurations. It is the first step for the problem diagnosis. Steps: Login on OpenShift Console Go to Administration -> Cluster Settings Click OpenShift Cluster Manager in Subscription section. It redirects the url to RedHat Hybrid Cloud Console Click Insights Advisor PID limit for docker \uf0c1 This settings how many processes can be run within one single container. If it is too small, it can cause folk bomb issue. E.g. db2w instance may be unavailable when there are thousands of connections/agents upcoming or Openshift Container Storage not behaving well with a large amount of PVCs. OOB value for OCP platforms: Platform Version Default Value IBM ROKS (4.8) 231239 AWS ROSA 1024 (Not Specified) Azure Self-Managed OCP 1024 Steps to check or update PID limit: $ oc debug node/$NODE_NAME $ chroot /host $ cat /etc/crio/crio.conf # add / modify the line \"pids_limit = <new value>\" # run belows commands to reboot services and worker nodes $ systemctl daemon-reload $ systemctl restart crio $ shutdown -r now HAProxy Router \uf0c1 Ingress Controller \uf0c1 Openshift HAProxy supports up to 20k connection per pod. Consider to scale up ingress pod for any app (like IoT) with a high volume connection workload. Scale up ingress controller command: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge Max Connection \uf0c1 One of the most important tunable parameters for HAProxy scalability is the maxconn parameter. The router can handle a maximum number of 20000 concurrent connections by using oc adm router --max-connections=xxxxx . This parameter will be impacted by node settings sysctl fs.nr_open and sysctl fs.file-max . HAproxy will not start if maxconn is high, but node setting is low. Note: OpenShift Container Platform no longer supports modifying Ingress Controller deployments by setting environment variables such as ROUTER_THREADS, ROUTER_DEFAULT_TUNNEL_TIMEOUT, ROUTER_DEFAULT_CLIENT_TIMEOUT, ROUTER_DEFAULT_SERVER_TIMEOUT, and RELOAD_INTERVAL. You can modify the Ingress Controller deployment, but if the Ingress Operator is enabled, the configuration is overwritten. Load Balance Algorithm \uf0c1 There are 3 load-balancing algorithms: source, roundrobin, and leastconn. leastconn is the default. Set up annotations for each route to change the default algorithm. e.g. haproxy.router.openshift.io/balance=roundrobin","title":"OpenShift Container Platform"},{"location":"mas/ocp/bestpractice/#openshift-container-platform","text":"","title":"OpenShift Container Platform"},{"location":"mas/ocp/bestpractice/#cluster-insights-advisor","text":"Highly recommend to use OpenShift cluster Insights Advisor that to check any issue related to the current version, nodes and mis-configurations. It is the first step for the problem diagnosis. Steps: Login on OpenShift Console Go to Administration -> Cluster Settings Click OpenShift Cluster Manager in Subscription section. It redirects the url to RedHat Hybrid Cloud Console Click Insights Advisor","title":"Cluster Insights Advisor"},{"location":"mas/ocp/bestpractice/#pid-limit-for-docker","text":"This settings how many processes can be run within one single container. If it is too small, it can cause folk bomb issue. E.g. db2w instance may be unavailable when there are thousands of connections/agents upcoming or Openshift Container Storage not behaving well with a large amount of PVCs. OOB value for OCP platforms: Platform Version Default Value IBM ROKS (4.8) 231239 AWS ROSA 1024 (Not Specified) Azure Self-Managed OCP 1024 Steps to check or update PID limit: $ oc debug node/$NODE_NAME $ chroot /host $ cat /etc/crio/crio.conf # add / modify the line \"pids_limit = <new value>\" # run belows commands to reboot services and worker nodes $ systemctl daemon-reload $ systemctl restart crio $ shutdown -r now","title":"PID limit for docker"},{"location":"mas/ocp/bestpractice/#haproxy-router","text":"","title":"HAProxy Router"},{"location":"mas/ocp/bestpractice/#ingress-controller","text":"Openshift HAProxy supports up to 20k connection per pod. Consider to scale up ingress pod for any app (like IoT) with a high volume connection workload. Scale up ingress controller command: oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge","title":"Ingress Controller"},{"location":"mas/ocp/bestpractice/#max-connection","text":"One of the most important tunable parameters for HAProxy scalability is the maxconn parameter. The router can handle a maximum number of 20000 concurrent connections by using oc adm router --max-connections=xxxxx . This parameter will be impacted by node settings sysctl fs.nr_open and sysctl fs.file-max . HAproxy will not start if maxconn is high, but node setting is low. Note: OpenShift Container Platform no longer supports modifying Ingress Controller deployments by setting environment variables such as ROUTER_THREADS, ROUTER_DEFAULT_TUNNEL_TIMEOUT, ROUTER_DEFAULT_CLIENT_TIMEOUT, ROUTER_DEFAULT_SERVER_TIMEOUT, and RELOAD_INTERVAL. You can modify the Ingress Controller deployment, but if the Ingress Operator is enabled, the configuration is overwritten.","title":"Max Connection"},{"location":"mas/ocp/bestpractice/#load-balance-algorithm","text":"There are 3 load-balancing algorithms: source, roundrobin, and leastconn. leastconn is the default. Set up annotations for each route to change the default algorithm. e.g. haproxy.router.openshift.io/balance=roundrobin","title":"Load Balance Algorithm"},{"location":"mas/sizing/guidance/","text":"Sizing Guidance \uf0c1 The sizing number in this page is based on a standard workload. Used as reference only. Sizing Calculation Sheet \uf0c1 Use Sizing Calculation Sheet for MAS sizing. Factors that impact the sizing consideration \uf0c1 storage operator: e.g. ocs, odf... cp4d services: e.g. db2w, watson studio... mongodb service kafka service OCS (OpenShift Container Storage) \uf0c1 If using OCS to manage the storage class, OCS service itself requires minimum 3 nodes with 14 core / 32G (Note: this is the total request amount, not per node). ODF (OpenShift Data Foundation) \uf0c1 3 OCP nodes will run ODF services. (NOTE: OCP clusters often contain additional OCP worker nodes which do not run ODF services.) Each OCP node running ODF services has:16 core / 64 GB memory CP4D/DB2W Minimum Resource Requirement \uf0c1 When running CP4D/DB2W on OpenShift's worknode, each instance requires at least 6.1 core and 18G ram . Note: an instance pod cannot be scheduled if the node's (total capacity - total limit) is less than 6.1 core or 18G ram, a dedicated worker node or external db is recommended. db2 operator is an alternative. MAS Resource Statistics \uf0c1 Namespace CPU Request CPU Limits Memory Request(GB) Memory Limits(GB) mas-masinst1-core 0.71 18.95 6.18 32.50 mas-masinst1-manage 2.5 11.1 4.04 17.00 mas-innovation-monitor 3.5 19.25 12.84 32.57 mas-innovation-iot 19.66 214.649 57.08 269.00 mongo 3 9 2.24 6.98 ibm-sls 0.12 2.6 0.56 4.50 kafka 9.2 31 30.38 30.38 ibm-common-services 0.81 1.92 1.30 2.73 OpenShift 3.2 28 Subtotal 42.7(core) 308.469(core) 142.62(GB) 395.66 (GB) Additional cost - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ocs* 14 32 14 32 cp4d (with 2 db2w instances)* 31.59 40.7 235.39 249.70 each additional manage pod* 1 6 2 10","title":"Sizing Guidance"},{"location":"mas/sizing/guidance/#sizing-guidance","text":"The sizing number in this page is based on a standard workload. Used as reference only.","title":"Sizing Guidance"},{"location":"mas/sizing/guidance/#sizing-calculation-sheet","text":"Use Sizing Calculation Sheet for MAS sizing.","title":"Sizing Calculation Sheet"},{"location":"mas/sizing/guidance/#factors-that-impact-the-sizing-consideration","text":"storage operator: e.g. ocs, odf... cp4d services: e.g. db2w, watson studio... mongodb service kafka service","title":"Factors that impact the sizing consideration"},{"location":"mas/sizing/guidance/#ocs-openshift-container-storage","text":"If using OCS to manage the storage class, OCS service itself requires minimum 3 nodes with 14 core / 32G (Note: this is the total request amount, not per node).","title":"OCS (OpenShift Container Storage)"},{"location":"mas/sizing/guidance/#odf-openshift-data-foundation","text":"3 OCP nodes will run ODF services. (NOTE: OCP clusters often contain additional OCP worker nodes which do not run ODF services.) Each OCP node running ODF services has:16 core / 64 GB memory","title":"ODF (OpenShift Data Foundation)"},{"location":"mas/sizing/guidance/#cp4ddb2w-minimum-resource-requirement","text":"When running CP4D/DB2W on OpenShift's worknode, each instance requires at least 6.1 core and 18G ram . Note: an instance pod cannot be scheduled if the node's (total capacity - total limit) is less than 6.1 core or 18G ram, a dedicated worker node or external db is recommended. db2 operator is an alternative.","title":"CP4D/DB2W Minimum Resource Requirement"},{"location":"mas/sizing/guidance/#mas-resource-statistics","text":"Namespace CPU Request CPU Limits Memory Request(GB) Memory Limits(GB) mas-masinst1-core 0.71 18.95 6.18 32.50 mas-masinst1-manage 2.5 11.1 4.04 17.00 mas-innovation-monitor 3.5 19.25 12.84 32.57 mas-innovation-iot 19.66 214.649 57.08 269.00 mongo 3 9 2.24 6.98 ibm-sls 0.12 2.6 0.56 4.50 kafka 9.2 31 30.38 30.38 ibm-common-services 0.81 1.92 1.30 2.73 OpenShift 3.2 28 Subtotal 42.7(core) 308.469(core) 142.62(GB) 395.66 (GB) Additional cost - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ocs* 14 32 14 32 cp4d (with 2 db2w instances)* 31.59 40.7 235.39 249.70 each additional manage pod* 1 6 2 10","title":"MAS Resource Statistics"},{"location":"maximo-7/bestpractice/","text":"Info The next version of Maximo 7 is MAS Manage 8.x. This page includes the best practice documents for Maximo EAM v7 only. The DB section in each best practice is still applicable to MAS Manage. Best Practices for Maximo 7.x \uf0c1 Maximo Best Practices for System Performance 7.6.x (Version 1.3) Maximo Anywhere 7.6.2 Best Practices (2018 Edition) Maximo Performance Tuning Tips: Additional tips on top of Best Practices for System Performance Maximo Best Practices for System Performance 7.5.x (Version 2.1) Improving Start Center performance in Maximo System Properties to Monitor and Troubleshoot Performance","title":"Best Practice"},{"location":"maximo-7/bestpractice/#best-practices-for-maximo-7x","text":"Maximo Best Practices for System Performance 7.6.x (Version 1.3) Maximo Anywhere 7.6.2 Best Practices (2018 Edition) Maximo Performance Tuning Tips: Additional tips on top of Best Practices for System Performance Maximo Best Practices for System Performance 7.5.x (Version 2.1) Improving Start Center performance in Maximo System Properties to Monitor and Troubleshoot Performance","title":"Best Practices for Maximo 7.x"},{"location":"pd/checklist/","text":"Performance Diagnosis \uf0c1 Info A monitoring system is strongly recommended to track the environment health and the quality of services. Diagnostic Utility \uf0c1 Scope Name Used for OCP OpenShift Monitoring Service OpenShift Cluster and MAS DB2 IBM DSM DB2 Historical and Realtime Troubleshooting DB2 db2top DB2 Realtime Troubleshooting Oracle AWR, StatsPack Historical Troubleshooting JVM IBM Support Assistant Heap Dump and GC Log Analysis JVM MAT JVM Dump Analysis Maximo PerfMon - Maximo UI Activity Tracing - Note : Enabling PerfMon may significantly degrade server performance. - Recommend for a single user with Dev/Test env only MongoDB mongotop MongoDB Realtime Troubleshooting HAR HTTP Archive Viewer HAR Analysis - for web page and client side (browser) performance SQL Poor SQL Online SQL Formatter SQL Squirrl Universal SQL Client SSL SSL Shopper Online certificate decode tool Performance Check List \uf0c1 check node status. e.g. any NOT Ready worker nodes if there is any pod or node cpu, memeory usage approaching to the limit? if there is any pod restarted many time recently? if there is any JVM Heapdump dump? if there is any JVM Hung Thread if there is any node or pod with a high system or IO wait (20%)? if there is any node memory, disk or pid pressure? if the response time is high (over 2 sec)? if any long running (over 2 sec) or high cpu cost query? if there is network bottleneck (e.g. load-balancer) is app server or db server busy? if app server is busy check the request, limit value for cpu, memory should replic memebers be increased? if db server is busy check cpu, memory, disk current usage and limit value check any utility in the background. e.g. backup check db lock check if there is any high cost query check disk performance","title":"Diagnosis Check List"},{"location":"pd/checklist/#performance-diagnosis","text":"Info A monitoring system is strongly recommended to track the environment health and the quality of services.","title":"Performance Diagnosis"},{"location":"pd/checklist/#diagnostic-utility","text":"Scope Name Used for OCP OpenShift Monitoring Service OpenShift Cluster and MAS DB2 IBM DSM DB2 Historical and Realtime Troubleshooting DB2 db2top DB2 Realtime Troubleshooting Oracle AWR, StatsPack Historical Troubleshooting JVM IBM Support Assistant Heap Dump and GC Log Analysis JVM MAT JVM Dump Analysis Maximo PerfMon - Maximo UI Activity Tracing - Note : Enabling PerfMon may significantly degrade server performance. - Recommend for a single user with Dev/Test env only MongoDB mongotop MongoDB Realtime Troubleshooting HAR HTTP Archive Viewer HAR Analysis - for web page and client side (browser) performance SQL Poor SQL Online SQL Formatter SQL Squirrl Universal SQL Client SSL SSL Shopper Online certificate decode tool","title":"Diagnostic Utility"},{"location":"pd/checklist/#performance-check-list","text":"check node status. e.g. any NOT Ready worker nodes if there is any pod or node cpu, memeory usage approaching to the limit? if there is any pod restarted many time recently? if there is any JVM Heapdump dump? if there is any JVM Hung Thread if there is any node or pod with a high system or IO wait (20%)? if there is any node memory, disk or pid pressure? if the response time is high (over 2 sec)? if any long running (over 2 sec) or high cpu cost query? if there is network bottleneck (e.g. load-balancer) is app server or db server busy? if app server is busy check the request, limit value for cpu, memory should replic memebers be increased? if db server is busy check cpu, memory, disk current usage and limit value check any utility in the background. e.g. backup check db lock check if there is any high cost query check disk performance","title":"Performance Check List"},{"location":"pd/ptbp/","text":"Performance Test Best Practice \uf0c1 There are 3 major performance tools used by the lab for performance test. Below are the best practice for each tool. Rational Performance Tester \uf0c1 Performance Test Best Practices for Rational Performance Tester LoadRunner \uf0c1 Performance Test Best Practices for LoadRunner JMeter \uf0c1 JMeter Best Practice","title":"Performance Test Best Practice"},{"location":"pd/ptbp/#performance-test-best-practice","text":"There are 3 major performance tools used by the lab for performance test. Below are the best practice for each tool.","title":"Performance Test Best Practice"},{"location":"pd/ptbp/#rational-performance-tester","text":"Performance Test Best Practices for Rational Performance Tester","title":"Rational Performance Tester"},{"location":"pd/ptbp/#loadrunner","text":"Performance Test Best Practices for LoadRunner","title":"LoadRunner"},{"location":"pd/ptbp/#jmeter","text":"JMeter Best Practice","title":"JMeter"}]}